{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StanfordCarsFGIC.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N8bV3Pq1qZha",
        "S1TEA1Olsz9f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bV3Pq1qZha"
      },
      "source": [
        "# Data preprocessing block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdmzDwheqhhz"
      },
      "source": [
        "## Spliting Data\n",
        "Split the data into 90% for training, 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQjHketcX4hi"
      },
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "total_list = os.listdir('/content/drive/My Drive/NCTU-1091深度學習/HW1/dataset/training_data/training_data')\n",
        "train_list, val_list = train_test_split(total_list, random_state=10, train_size=0.9)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIAAGYGJqxcR"
      },
      "source": [
        "## Organize Dataset\n",
        "Create image corresponding class map dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9wIWCHZpMnF"
      },
      "source": [
        "import csv\n",
        "with open('/content/drive/My Drive/NCTU-1091深度學習/HW1/dataset/training_labels.csv', newline='') as csvfile:\n",
        "  total_dict = {}\n",
        "  rows = csv.reader(csvfile)\n",
        "  index = next(rows)\n",
        "  for row in rows:\n",
        "    if row[1] not in total_dict:\n",
        "      total_dict[row[1]] = []\n",
        "      total_dict[row[1]].append(row[0])\n",
        "    else:\n",
        "      total_dict[row[1]].append(row[0])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78mvuQN-rlPV"
      },
      "source": [
        "Move images of the same class into same train or validation class folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fecrKCeHrfvg",
        "outputId": "d299f6e4-cd7f-4593-e7ad-fd476961a96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import shutil\n",
        "target_path = '/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/'\n",
        "src_path = '/content/drive/My Drive/NCTU-1091深度學習/HW1/dataset/training_data/training_data/'\n",
        "for class_name in total_dict.keys():\n",
        "  if os.path.exists(r'/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/train/{}'.format(class_name)):\n",
        "    continue\n",
        "  if '/' in class_name:\n",
        "    print('this class is illegal:', class_name)\n",
        "    for img_number in total_dict[class_name]:\n",
        "      img_number = img_number+'.jpg'\n",
        "      if img_number in train_list:\n",
        "        shutil.copy(src_path+img_number, '/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/train/Ram C V Cargo Van Minivan 2012/'+img_number)\n",
        "      else:\n",
        "        shutil.copy(src_path+img_number, '/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/test/Ram C V Cargo Van Minivan 2012/'+img_number)\n",
        "    print(class_name, 'done!')\n",
        "    continue\n",
        "  os.mkdir(r'/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/train/{}'.format(class_name))\n",
        "  os.mkdir(r'/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/test/{}'.format(class_name))\n",
        "  for img_number in total_dict[class_name]:\n",
        "    img_number = img_number+'.jpg'\n",
        "    if img_number in train_list:\n",
        "      shutil.copy(src_path+img_number, target_path+r'train/{}'.format(class_name+'/'+img_number))\n",
        "    else:\n",
        "      shutil.copy(src_path+img_number, target_path+r'test/{}'.format(class_name+'/'+img_number))\n",
        "  print(class_name, 'done!')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this class is illegal: Ram C/V Cargo Van Minivan 2012\n",
            "Ram C/V Cargo Van Minivan 2012 done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwA91OTFsRH-"
      },
      "source": [
        "# Training block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwOsXZBtsbac"
      },
      "source": [
        "## Train model\n",
        "Train model and save it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPrrIy__GZCv",
        "outputId": "63f41810-8d27-4f3b-cffd-66ee3169b911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsYDMH-DGmic",
        "outputId": "d6406a72-6d8a-4105-b79c-17c51b7870de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train.py\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "==> Preparing data..\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "Step: 0 | Loss1: 5.377 | Loss2: 5.48127 | Loss3: 5.30428 | Loss_concat: 11.07520 | Loss: 27.238 | Acc: 0.000% (0/16)\n",
            "Step: 50 | Loss1: 5.312 | Loss2: 5.41644 | Loss3: 5.29225 | Loss_concat: 10.57218 | Loss: 26.593 | Acc: 2.206% (18/816)\n",
            "Step: 100 | Loss1: 5.266 | Loss2: 5.32563 | Loss3: 5.15033 | Loss_concat: 10.13082 | Loss: 25.872 | Acc: 4.208% (68/1616)\n",
            "Step: 150 | Loss1: 5.199 | Loss2: 5.22773 | Loss3: 4.99455 | Loss_concat: 9.61480 | Loss: 25.036 | Acc: 6.581% (159/2416)\n",
            "Step: 200 | Loss1: 5.131 | Loss2: 5.10550 | Loss3: 4.82906 | Loss_concat: 9.07416 | Loss: 24.139 | Acc: 9.981% (321/3216)\n",
            "Step: 250 | Loss1: 5.080 | Loss2: 4.97664 | Loss3: 4.66054 | Loss_concat: 8.55027 | Loss: 23.267 | Acc: 13.670% (549/4016)\n",
            "Step: 300 | Loss1: 5.025 | Loss2: 4.85595 | Loss3: 4.51178 | Loss_concat: 8.09036 | Loss: 22.483 | Acc: 17.525% (844/4816)\n",
            "Step: 350 | Loss1: 4.971 | Loss2: 4.73485 | Loss3: 4.36812 | Loss_concat: 7.65187 | Loss: 21.725 | Acc: 21.563% (1211/5616)\n",
            "Step: 400 | Loss1: 4.914 | Loss2: 4.61096 | Loss3: 4.22948 | Loss_concat: 7.24293 | Loss: 20.997 | Acc: 25.608% (1643/6416)\n",
            "Step: 450 | Loss1: 4.851 | Loss2: 4.48575 | Loss3: 4.09610 | Loss_concat: 6.84690 | Loss: 20.279 | Acc: 29.739% (2146/7216)\n",
            "Step: 500 | Loss1: 4.796 | Loss2: 4.36828 | Loss3: 3.96343 | Loss_concat: 6.48891 | Loss: 19.617 | Acc: 33.471% (2683/8016)\n",
            "Step: 550 | Loss1: 4.741 | Loss2: 4.25864 | Loss3: 3.84299 | Loss_concat: 6.17122 | Loss: 19.014 | Acc: 36.797% (3244/8816)\n",
            "Step: 600 | Loss1: 4.685 | Loss2: 4.15291 | Loss3: 3.73282 | Loss_concat: 5.88794 | Loss: 18.459 | Acc: 39.611% (3809/9616)\n",
            "/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/utils.py:85: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
            "Step: 0 | Loss: 1.095 | Acc: 33.333% (1/3) |Combined Acc: 33.333% (1/3)\n",
            "Step: 50 | Loss: 1.220 | Acc: 68.627% (105/153) |Combined Acc: 63.399% (97/153)\n",
            "Step: 100 | Loss: 1.113 | Acc: 71.947% (218/303) |Combined Acc: 67.987% (206/303)\n",
            "Step: 150 | Loss: 1.137 | Acc: 70.861% (321/453) |Combined Acc: 68.653% (311/453)\n",
            "Step: 200 | Loss: 1.162 | Acc: 70.647% (426/603) |Combined Acc: 67.828% (409/603)\n",
            "Step: 250 | Loss: 1.223 | Acc: 68.127% (513/753) |Combined Acc: 65.471% (493/753)\n",
            "Step: 300 | Loss: 1.231 | Acc: 67.996% (614/903) |Combined Acc: 64.895% (586/903)\n",
            "Step: 350 | Loss: 1.221 | Acc: 68.566% (722/1053) |Combined Acc: 65.907% (694/1053)\n",
            "\n",
            "Epoch: 1\n",
            "Step: 0 | Loss1: 3.700 | Loss2: 2.60232 | Loss3: 1.99610 | Loss_concat: 1.85135 | Loss: 10.150 | Acc: 81.250% (13/16)\n",
            "Step: 50 | Loss1: 3.634 | Loss2: 2.45171 | Loss3: 1.95791 | Loss_concat: 1.66671 | Loss: 9.711 | Acc: 85.294% (696/816)\n",
            "Step: 100 | Loss1: 3.571 | Loss2: 2.39721 | Loss3: 1.92704 | Loss_concat: 1.62014 | Loss: 9.516 | Acc: 85.705% (1385/1616)\n",
            "Step: 150 | Loss1: 3.576 | Loss2: 2.35080 | Loss3: 1.88453 | Loss_concat: 1.57962 | Loss: 9.391 | Acc: 85.969% (2077/2416)\n",
            "Step: 200 | Loss1: 3.547 | Loss2: 2.30250 | Loss3: 1.83911 | Loss_concat: 1.55207 | Loss: 9.241 | Acc: 85.852% (2761/3216)\n",
            "Step: 250 | Loss1: 3.516 | Loss2: 2.26712 | Loss3: 1.81163 | Loss_concat: 1.53635 | Loss: 9.131 | Acc: 85.682% (3441/4016)\n",
            "Step: 300 | Loss1: 3.474 | Loss2: 2.21324 | Loss3: 1.76263 | Loss_concat: 1.49218 | Loss: 8.943 | Acc: 85.777% (4131/4816)\n",
            "Step: 350 | Loss1: 3.437 | Loss2: 2.16075 | Loss3: 1.71704 | Loss_concat: 1.43557 | Loss: 8.750 | Acc: 86.467% (4856/5616)\n",
            "Step: 400 | Loss1: 3.390 | Loss2: 2.11400 | Loss3: 1.68013 | Loss_concat: 1.40810 | Loss: 8.593 | Acc: 86.674% (5561/6416)\n",
            "Step: 450 | Loss1: 3.351 | Loss2: 2.07718 | Loss3: 1.64660 | Loss_concat: 1.38285 | Loss: 8.458 | Acc: 86.835% (6266/7216)\n",
            "Step: 500 | Loss1: 3.314 | Loss2: 2.03815 | Loss3: 1.61474 | Loss_concat: 1.35842 | Loss: 8.325 | Acc: 86.989% (6973/8016)\n",
            "Step: 550 | Loss1: 3.273 | Loss2: 1.99071 | Loss3: 1.57658 | Loss_concat: 1.32094 | Loss: 8.161 | Acc: 87.319% (7698/8816)\n",
            "Step: 600 | Loss1: 3.241 | Loss2: 1.95293 | Loss3: 1.55075 | Loss_concat: 1.29732 | Loss: 8.042 | Acc: 87.531% (8417/9616)\n",
            "Step: 0 | Loss: 0.431 | Acc: 100.000% (3/3) |Combined Acc: 66.667% (2/3)\n",
            "Step: 50 | Loss: 0.540 | Acc: 86.275% (132/153) |Combined Acc: 83.007% (127/153)\n",
            "Step: 100 | Loss: 0.536 | Acc: 86.139% (261/303) |Combined Acc: 84.818% (257/303)\n",
            "Step: 150 | Loss: 0.562 | Acc: 84.547% (383/453) |Combined Acc: 83.002% (376/453)\n",
            "Step: 200 | Loss: 0.559 | Acc: 84.743% (511/603) |Combined Acc: 83.748% (505/603)\n",
            "Step: 250 | Loss: 0.578 | Acc: 84.197% (634/753) |Combined Acc: 83.267% (627/753)\n",
            "Step: 300 | Loss: 0.572 | Acc: 84.385% (762/903) |Combined Acc: 83.389% (753/903)\n",
            "Step: 350 | Loss: 0.555 | Acc: 85.185% (897/1053) |Combined Acc: 84.141% (886/1053)\n",
            "\n",
            "Epoch: 2\n",
            "Step: 0 | Loss1: 2.867 | Loss2: 1.89176 | Loss3: 1.29814 | Loss_concat: 1.28494 | Loss: 7.341 | Acc: 93.750% (15/16)\n",
            "Step: 50 | Loss1: 2.493 | Loss2: 1.19939 | Loss3: 0.95167 | Loss_concat: 0.57532 | Loss: 5.219 | Acc: 94.608% (772/816)\n",
            "Step: 100 | Loss1: 2.469 | Loss2: 1.18656 | Loss3: 0.92259 | Loss_concat: 0.54568 | Loss: 5.123 | Acc: 95.421% (1542/1616)\n",
            "Step: 150 | Loss1: 2.441 | Loss2: 1.17137 | Loss3: 0.92422 | Loss_concat: 0.54550 | Loss: 5.082 | Acc: 95.488% (2307/2416)\n",
            "Step: 200 | Loss1: 2.426 | Loss2: 1.16376 | Loss3: 0.90597 | Loss_concat: 0.55937 | Loss: 5.055 | Acc: 95.025% (3056/3216)\n",
            "Step: 250 | Loss1: 2.402 | Loss2: 1.13914 | Loss3: 0.88548 | Loss_concat: 0.55571 | Loss: 4.982 | Acc: 94.995% (3815/4016)\n",
            "Step: 300 | Loss1: 2.380 | Loss2: 1.12406 | Loss3: 0.87093 | Loss_concat: 0.54758 | Loss: 4.922 | Acc: 95.079% (4579/4816)\n",
            "Step: 350 | Loss1: 2.354 | Loss2: 1.11336 | Loss3: 0.85778 | Loss_concat: 0.54574 | Loss: 4.871 | Acc: 94.907% (5330/5616)\n",
            "Step: 400 | Loss1: 2.334 | Loss2: 1.09841 | Loss3: 0.84850 | Loss_concat: 0.54700 | Loss: 4.828 | Acc: 94.872% (6087/6416)\n",
            "Step: 450 | Loss1: 2.300 | Loss2: 1.07878 | Loss3: 0.83034 | Loss_concat: 0.53725 | Loss: 4.746 | Acc: 94.956% (6852/7216)\n",
            "Step: 500 | Loss1: 2.271 | Loss2: 1.05761 | Loss3: 0.81499 | Loss_concat: 0.52956 | Loss: 4.673 | Acc: 95.035% (7618/8016)\n",
            "Step: 550 | Loss1: 2.252 | Loss2: 1.04621 | Loss3: 0.80850 | Loss_concat: 0.52482 | Loss: 4.632 | Acc: 95.145% (8388/8816)\n",
            "Step: 600 | Loss1: 2.233 | Loss2: 1.03383 | Loss3: 0.79952 | Loss_concat: 0.52421 | Loss: 4.591 | Acc: 95.123% (9147/9616)\n",
            "Step: 0 | Loss: 0.436 | Acc: 100.000% (3/3) |Combined Acc: 66.667% (2/3)\n",
            "Step: 50 | Loss: 0.512 | Acc: 88.889% (136/153) |Combined Acc: 87.582% (134/153)\n",
            "Step: 100 | Loss: 0.448 | Acc: 89.769% (272/303) |Combined Acc: 89.769% (272/303)\n",
            "Step: 150 | Loss: 0.423 | Acc: 89.183% (404/453) |Combined Acc: 88.742% (402/453)\n",
            "Step: 200 | Loss: 0.417 | Acc: 89.386% (539/603) |Combined Acc: 88.391% (533/603)\n",
            "Step: 250 | Loss: 0.417 | Acc: 89.110% (671/753) |Combined Acc: 88.313% (665/753)\n",
            "Step: 300 | Loss: 0.419 | Acc: 88.815% (802/903) |Combined Acc: 88.040% (795/903)\n",
            "Step: 350 | Loss: 0.418 | Acc: 88.699% (934/1053) |Combined Acc: 88.129% (928/1053)\n",
            "\n",
            "Epoch: 3\n",
            "Step: 0 | Loss1: 1.734 | Loss2: 0.68802 | Loss3: 0.55338 | Loss_concat: 0.21855 | Loss: 3.194 | Acc: 100.000% (16/16)\n",
            "Step: 50 | Loss1: 1.817 | Loss2: 0.74387 | Loss3: 0.57227 | Loss_concat: 0.24994 | Loss: 3.383 | Acc: 98.897% (807/816)\n",
            "Step: 100 | Loss1: 1.758 | Loss2: 0.72072 | Loss3: 0.55976 | Loss_concat: 0.25971 | Loss: 3.298 | Acc: 98.453% (1591/1616)\n",
            "Step: 150 | Loss1: 1.721 | Loss2: 0.70603 | Loss3: 0.54466 | Loss_concat: 0.25678 | Loss: 3.229 | Acc: 98.469% (2379/2416)\n",
            "Step: 200 | Loss1: 1.696 | Loss2: 0.69342 | Loss3: 0.53200 | Loss_concat: 0.26119 | Loss: 3.182 | Acc: 98.165% (3157/3216)\n",
            "Step: 250 | Loss1: 1.684 | Loss2: 0.68812 | Loss3: 0.52668 | Loss_concat: 0.26866 | Loss: 3.168 | Acc: 98.008% (3936/4016)\n",
            "Step: 300 | Loss1: 1.671 | Loss2: 0.68290 | Loss3: 0.52213 | Loss_concat: 0.26897 | Loss: 3.145 | Acc: 97.965% (4718/4816)\n",
            "Step: 350 | Loss1: 1.660 | Loss2: 0.67805 | Loss3: 0.52335 | Loss_concat: 0.27384 | Loss: 3.136 | Acc: 97.828% (5494/5616)\n",
            "Step: 400 | Loss1: 1.659 | Loss2: 0.67818 | Loss3: 0.52152 | Loss_concat: 0.27494 | Loss: 3.134 | Acc: 97.802% (6275/6416)\n",
            "Step: 450 | Loss1: 1.653 | Loss2: 0.67879 | Loss3: 0.51947 | Loss_concat: 0.28846 | Loss: 3.139 | Acc: 97.519% (7037/7216)\n",
            "Step: 500 | Loss1: 1.644 | Loss2: 0.67380 | Loss3: 0.51572 | Loss_concat: 0.29090 | Loss: 3.124 | Acc: 97.480% (7814/8016)\n",
            "Step: 550 | Loss1: 1.635 | Loss2: 0.67372 | Loss3: 0.51594 | Loss_concat: 0.29687 | Loss: 3.121 | Acc: 97.368% (8584/8816)\n",
            "Step: 600 | Loss1: 1.626 | Loss2: 0.66744 | Loss3: 0.51176 | Loss_concat: 0.29452 | Loss: 3.100 | Acc: 97.431% (9369/9616)\n",
            "Step: 0 | Loss: 0.013 | Acc: 100.000% (3/3) |Combined Acc: 100.000% (3/3)\n",
            "Step: 50 | Loss: 0.432 | Acc: 88.889% (136/153) |Combined Acc: 88.235% (135/153)\n",
            "Step: 100 | Loss: 0.431 | Acc: 89.439% (271/303) |Combined Acc: 89.109% (270/303)\n",
            "Step: 150 | Loss: 0.436 | Acc: 89.183% (404/453) |Combined Acc: 88.962% (403/453)\n",
            "Step: 200 | Loss: 0.399 | Acc: 90.216% (544/603) |Combined Acc: 89.884% (542/603)\n",
            "Step: 250 | Loss: 0.392 | Acc: 90.305% (680/753) |Combined Acc: 90.040% (678/753)\n",
            "Step: 300 | Loss: 0.384 | Acc: 90.808% (820/903) |Combined Acc: 90.808% (820/903)\n",
            "Step: 350 | Loss: 0.385 | Acc: 90.313% (951/1053) |Combined Acc: 90.408% (952/1053)\n",
            "\n",
            "Epoch: 4\n",
            "Step: 0 | Loss1: 1.249 | Loss2: 0.54159 | Loss3: 0.41077 | Loss_concat: 0.22985 | Loss: 2.431 | Acc: 93.750% (15/16)\n",
            "Step: 50 | Loss1: 1.297 | Loss2: 0.47151 | Loss3: 0.38012 | Loss_concat: 0.18238 | Loss: 2.331 | Acc: 98.407% (803/816)\n",
            "Step: 100 | Loss1: 1.302 | Loss2: 0.46143 | Loss3: 0.35539 | Loss_concat: 0.17560 | Loss: 2.294 | Acc: 98.639% (1594/1616)\n",
            "Step: 150 | Loss1: 1.297 | Loss2: 0.47302 | Loss3: 0.35833 | Loss_concat: 0.17687 | Loss: 2.306 | Acc: 98.551% (2381/2416)\n",
            "Step: 200 | Loss1: 1.302 | Loss2: 0.48400 | Loss3: 0.37065 | Loss_concat: 0.18693 | Loss: 2.344 | Acc: 98.352% (3163/3216)\n",
            "Step: 250 | Loss1: 1.304 | Loss2: 0.49221 | Loss3: 0.37146 | Loss_concat: 0.18763 | Loss: 2.355 | Acc: 98.431% (3953/4016)\n",
            "Step: 300 | Loss1: 1.281 | Loss2: 0.48323 | Loss3: 0.36810 | Loss_concat: 0.19029 | Loss: 2.322 | Acc: 98.443% (4741/4816)\n",
            "Step: 350 | Loss1: 1.272 | Loss2: 0.48239 | Loss3: 0.36618 | Loss_concat: 0.19137 | Loss: 2.312 | Acc: 98.326% (5522/5616)\n",
            "Step: 400 | Loss1: 1.262 | Loss2: 0.47781 | Loss3: 0.36701 | Loss_concat: 0.19823 | Loss: 2.305 | Acc: 98.239% (6303/6416)\n",
            "Step: 450 | Loss1: 1.258 | Loss2: 0.47668 | Loss3: 0.36388 | Loss_concat: 0.19715 | Loss: 2.296 | Acc: 98.295% (7093/7216)\n",
            "Step: 500 | Loss1: 1.248 | Loss2: 0.47263 | Loss3: 0.36280 | Loss_concat: 0.20019 | Loss: 2.283 | Acc: 98.253% (7876/8016)\n",
            "Step: 550 | Loss1: 1.240 | Loss2: 0.47003 | Loss3: 0.35980 | Loss_concat: 0.19870 | Loss: 2.268 | Acc: 98.310% (8667/8816)\n",
            "Step: 600 | Loss1: 1.237 | Loss2: 0.47052 | Loss3: 0.35971 | Loss_concat: 0.19786 | Loss: 2.265 | Acc: 98.305% (9453/9616)\n",
            "Step: 0 | Loss: 0.033 | Acc: 100.000% (3/3) |Combined Acc: 100.000% (3/3)\n",
            "Step: 50 | Loss: 0.422 | Acc: 88.889% (136/153) |Combined Acc: 88.889% (136/153)\n",
            "Step: 100 | Loss: 0.306 | Acc: 91.749% (278/303) |Combined Acc: 92.079% (279/303)\n",
            "Step: 150 | Loss: 0.324 | Acc: 91.391% (414/453) |Combined Acc: 91.611% (415/453)\n",
            "Step: 200 | Loss: 0.310 | Acc: 91.542% (552/603) |Combined Acc: 92.040% (555/603)\n",
            "Step: 250 | Loss: 0.309 | Acc: 91.368% (688/753) |Combined Acc: 91.633% (690/753)\n",
            "Step: 300 | Loss: 0.317 | Acc: 91.141% (823/903) |Combined Acc: 91.473% (826/903)\n",
            "Step: 350 | Loss: 0.334 | Acc: 90.788% (956/1053) |Combined Acc: 90.978% (958/1053)\n",
            "\n",
            "Epoch: 5\n",
            "Step: 0 | Loss1: 0.876 | Loss2: 0.29654 | Loss3: 0.31114 | Loss_concat: 0.16377 | Loss: 1.647 | Acc: 100.000% (16/16)\n",
            "Step: 50 | Loss1: 0.952 | Loss2: 0.35481 | Loss3: 0.27572 | Loss_concat: 0.11244 | Loss: 1.695 | Acc: 100.000% (816/816)\n",
            "Step: 100 | Loss1: 1.007 | Loss2: 0.37188 | Loss3: 0.27483 | Loss_concat: 0.13552 | Loss: 1.789 | Acc: 99.257% (1604/1616)\n",
            "Step: 150 | Loss1: 0.999 | Loss2: 0.36438 | Loss3: 0.26682 | Loss_concat: 0.13181 | Loss: 1.762 | Acc: 99.172% (2396/2416)\n",
            "Step: 200 | Loss1: 0.999 | Loss2: 0.36947 | Loss3: 0.26568 | Loss_concat: 0.13188 | Loss: 1.766 | Acc: 99.129% (3188/3216)\n",
            "Step: 250 | Loss1: 0.998 | Loss2: 0.36886 | Loss3: 0.26745 | Loss_concat: 0.13671 | Loss: 1.771 | Acc: 99.054% (3978/4016)\n",
            "Step: 300 | Loss1: 1.001 | Loss2: 0.36919 | Loss3: 0.26841 | Loss_concat: 0.13603 | Loss: 1.775 | Acc: 99.066% (4771/4816)\n",
            "Step: 350 | Loss1: 1.003 | Loss2: 0.37212 | Loss3: 0.26917 | Loss_concat: 0.13431 | Loss: 1.778 | Acc: 99.092% (5565/5616)\n",
            "Step: 400 | Loss1: 0.995 | Loss2: 0.36750 | Loss3: 0.26775 | Loss_concat: 0.13773 | Loss: 1.768 | Acc: 98.987% (6351/6416)\n",
            "Step: 450 | Loss1: 0.991 | Loss2: 0.36506 | Loss3: 0.26684 | Loss_concat: 0.14242 | Loss: 1.765 | Acc: 98.919% (7138/7216)\n",
            "Step: 500 | Loss1: 0.984 | Loss2: 0.36477 | Loss3: 0.26892 | Loss_concat: 0.14455 | Loss: 1.762 | Acc: 98.852% (7924/8016)\n",
            "Step: 550 | Loss1: 0.982 | Loss2: 0.36521 | Loss3: 0.26963 | Loss_concat: 0.14704 | Loss: 1.764 | Acc: 98.809% (8711/8816)\n",
            "Step: 600 | Loss1: 0.980 | Loss2: 0.36425 | Loss3: 0.27011 | Loss_concat: 0.14990 | Loss: 1.765 | Acc: 98.783% (9499/9616)\n",
            "\n",
            "Epoch: 6\n",
            "Step: 0 | Loss1: 0.535 | Loss2: 0.27942 | Loss3: 0.25762 | Loss_concat: 0.10024 | Loss: 1.172 | Acc: 100.000% (16/16)\n",
            "Step: 50 | Loss1: 0.790 | Loss2: 0.28127 | Loss3: 0.21249 | Loss_concat: 0.11231 | Loss: 1.396 | Acc: 99.632% (813/816)\n",
            "Step: 100 | Loss1: 0.799 | Loss2: 0.27409 | Loss3: 0.20853 | Loss_concat: 0.10172 | Loss: 1.384 | Acc: 99.691% (1611/1616)\n",
            "Step: 150 | Loss1: 0.832 | Loss2: 0.28873 | Loss3: 0.21053 | Loss_concat: 0.10301 | Loss: 1.435 | Acc: 99.586% (2406/2416)\n",
            "Step: 200 | Loss1: 0.823 | Loss2: 0.29255 | Loss3: 0.21150 | Loss_concat: 0.11074 | Loss: 1.438 | Acc: 99.316% (3194/3216)\n",
            "Step: 250 | Loss1: 0.823 | Loss2: 0.29742 | Loss3: 0.21099 | Loss_concat: 0.11247 | Loss: 1.443 | Acc: 99.278% (3987/4016)\n",
            "Step: 300 | Loss1: 0.822 | Loss2: 0.29348 | Loss3: 0.21279 | Loss_concat: 0.11829 | Loss: 1.447 | Acc: 99.128% (4774/4816)\n",
            "Step: 350 | Loss1: 0.824 | Loss2: 0.29622 | Loss3: 0.21264 | Loss_concat: 0.12064 | Loss: 1.454 | Acc: 99.110% (5566/5616)\n",
            "Step: 400 | Loss1: 0.822 | Loss2: 0.29689 | Loss3: 0.21359 | Loss_concat: 0.11991 | Loss: 1.453 | Acc: 99.127% (6360/6416)\n",
            "Step: 450 | Loss1: 0.815 | Loss2: 0.29777 | Loss3: 0.21299 | Loss_concat: 0.11942 | Loss: 1.446 | Acc: 99.141% (7154/7216)\n",
            "Step: 500 | Loss1: 0.813 | Loss2: 0.29711 | Loss3: 0.21358 | Loss_concat: 0.12187 | Loss: 1.446 | Acc: 99.114% (7945/8016)\n",
            "Step: 550 | Loss1: 0.808 | Loss2: 0.29704 | Loss3: 0.21523 | Loss_concat: 0.12205 | Loss: 1.442 | Acc: 99.104% (8737/8816)\n",
            "Step: 600 | Loss1: 0.815 | Loss2: 0.29999 | Loss3: 0.21776 | Loss_concat: 0.12339 | Loss: 1.456 | Acc: 99.106% (9530/9616)\n",
            "\n",
            "Epoch: 7\n",
            "Step: 0 | Loss1: 0.488 | Loss2: 0.16930 | Loss3: 0.16310 | Loss_concat: 0.07798 | Loss: 0.898 | Acc: 100.000% (16/16)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 166, in <module>\n",
            "    model_path='')         # the saved model where you want to resume the training\n",
            "  File \"train.py\", line 116, in train\n",
            "    concat_loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 185, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 127, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p0pwJYmssTQ"
      },
      "source": [
        "# Inference block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1TEA1Olsz9f"
      },
      "source": [
        "## Build class index map\n",
        "the classes.txt is build using method of torchvision.datasets.ImageFolder.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9d1YvZqt-b",
        "outputId": "ddac6c17-3144-4041-ee34-c13965ae394d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open('/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/classes.txt', 'r') as f:\n",
        "  classes = f.read().split('\\n')\n",
        "  clidx_map = {}\n",
        "  for i in classes:\n",
        "    map = i.split(',')\n",
        "    if len(map) > 1:\n",
        "      clidx_map[map[1]] = map[0]\n",
        "clidx_map"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 'AM General Hummer SUV 2000',\n",
              " '1': 'Acura Integra Type R 2001',\n",
              " '10': 'Aston Martin Virage Coupe 2012',\n",
              " '100': 'Ferrari 458 Italia Convertible 2012',\n",
              " '101': 'Ferrari 458 Italia Coupe 2012',\n",
              " '102': 'Ferrari California Convertible 2012',\n",
              " '103': 'Ferrari FF Coupe 2012',\n",
              " '104': 'Fisker Karma Sedan 2012',\n",
              " '105': 'Ford E-Series Wagon Van 2012',\n",
              " '106': 'Ford Edge SUV 2012',\n",
              " '107': 'Ford Expedition EL SUV 2009',\n",
              " '108': 'Ford F-150 Regular Cab 2007',\n",
              " '109': 'Ford F-150 Regular Cab 2012',\n",
              " '11': 'Audi 100 Sedan 1994',\n",
              " '110': 'Ford F-450 Super Duty Crew Cab 2012',\n",
              " '111': 'Ford Fiesta Sedan 2012',\n",
              " '112': 'Ford Focus Sedan 2007',\n",
              " '113': 'Ford Freestar Minivan 2007',\n",
              " '114': 'Ford GT Coupe 2006',\n",
              " '115': 'Ford Mustang Convertible 2007',\n",
              " '116': 'Ford Ranger SuperCab 2011',\n",
              " '117': 'GMC Acadia SUV 2012',\n",
              " '118': 'GMC Canyon Extended Cab 2012',\n",
              " '119': 'GMC Savana Van 2012',\n",
              " '12': 'Audi 100 Wagon 1994',\n",
              " '120': 'GMC Terrain SUV 2012',\n",
              " '121': 'GMC Yukon Hybrid SUV 2012',\n",
              " '122': 'Geo Metro Convertible 1993',\n",
              " '123': 'HUMMER H2 SUT Crew Cab 2009',\n",
              " '124': 'HUMMER H3T Crew Cab 2010',\n",
              " '125': 'Honda Accord Coupe 2012',\n",
              " '126': 'Honda Accord Sedan 2012',\n",
              " '127': 'Honda Odyssey Minivan 2007',\n",
              " '128': 'Honda Odyssey Minivan 2012',\n",
              " '129': 'Hyundai Accent Sedan 2012',\n",
              " '13': 'Audi A5 Coupe 2012',\n",
              " '130': 'Hyundai Azera Sedan 2012',\n",
              " '131': 'Hyundai Elantra Sedan 2007',\n",
              " '132': 'Hyundai Elantra Touring Hatchback 2012',\n",
              " '133': 'Hyundai Genesis Sedan 2012',\n",
              " '134': 'Hyundai Santa Fe SUV 2012',\n",
              " '135': 'Hyundai Sonata Hybrid Sedan 2012',\n",
              " '136': 'Hyundai Sonata Sedan 2012',\n",
              " '137': 'Hyundai Tucson SUV 2012',\n",
              " '138': 'Hyundai Veloster Hatchback 2012',\n",
              " '139': 'Hyundai Veracruz SUV 2012',\n",
              " '14': 'Audi R8 Coupe 2012',\n",
              " '140': 'Infiniti G Coupe IPL 2012',\n",
              " '141': 'Infiniti QX56 SUV 2011',\n",
              " '142': 'Isuzu Ascender SUV 2008',\n",
              " '143': 'Jaguar XK XKR 2012',\n",
              " '144': 'Jeep Compass SUV 2012',\n",
              " '145': 'Jeep Grand Cherokee SUV 2012',\n",
              " '146': 'Jeep Liberty SUV 2012',\n",
              " '147': 'Jeep Patriot SUV 2012',\n",
              " '148': 'Jeep Wrangler SUV 2012',\n",
              " '149': 'Lamborghini Aventador Coupe 2012',\n",
              " '15': 'Audi RS 4 Convertible 2008',\n",
              " '150': 'Lamborghini Diablo Coupe 2001',\n",
              " '151': 'Lamborghini Gallardo LP 570-4 Superleggera 2012',\n",
              " '152': 'Lamborghini Reventon Coupe 2008',\n",
              " '153': 'Land Rover LR2 SUV 2012',\n",
              " '154': 'Land Rover Range Rover SUV 2012',\n",
              " '155': 'Lincoln Town Car Sedan 2011',\n",
              " '156': 'MINI Cooper Roadster Convertible 2012',\n",
              " '157': 'Maybach Landaulet Convertible 2012',\n",
              " '158': 'Mazda Tribute SUV 2011',\n",
              " '159': 'McLaren MP4-12C Coupe 2012',\n",
              " '16': 'Audi S4 Sedan 2007',\n",
              " '160': 'Mercedes-Benz 300-Class Convertible 1993',\n",
              " '161': 'Mercedes-Benz C-Class Sedan 2012',\n",
              " '162': 'Mercedes-Benz E-Class Sedan 2012',\n",
              " '163': 'Mercedes-Benz S-Class Sedan 2012',\n",
              " '164': 'Mercedes-Benz SL-Class Coupe 2009',\n",
              " '165': 'Mercedes-Benz Sprinter Van 2012',\n",
              " '166': 'Mitsubishi Lancer Sedan 2012',\n",
              " '167': 'Nissan 240SX Coupe 1998',\n",
              " '168': 'Nissan Juke Hatchback 2012',\n",
              " '169': 'Nissan Leaf Hatchback 2012',\n",
              " '17': 'Audi S4 Sedan 2012',\n",
              " '170': 'Nissan NV Passenger Van 2012',\n",
              " '171': 'Plymouth Neon Coupe 1999',\n",
              " '172': 'Porsche Panamera Sedan 2012',\n",
              " '173': 'Ram C/V Cargo Van Minivan 2012',\n",
              " '174': 'Rolls-Royce Ghost Sedan 2012',\n",
              " '175': 'Rolls-Royce Phantom Drophead Coupe Convertible 2012',\n",
              " '176': 'Rolls-Royce Phantom Sedan 2012',\n",
              " '177': 'Scion xD Hatchback 2012',\n",
              " '178': 'Spyker C8 Convertible 2009',\n",
              " '179': 'Spyker C8 Coupe 2009',\n",
              " '18': 'Audi S5 Convertible 2012',\n",
              " '180': 'Suzuki Aerio Sedan 2007',\n",
              " '181': 'Suzuki Kizashi Sedan 2012',\n",
              " '182': 'Suzuki SX4 Hatchback 2012',\n",
              " '183': 'Suzuki SX4 Sedan 2012',\n",
              " '184': 'Tesla Model S Sedan 2012',\n",
              " '185': 'Toyota 4Runner SUV 2012',\n",
              " '186': 'Toyota Camry Sedan 2012',\n",
              " '187': 'Toyota Corolla Sedan 2012',\n",
              " '188': 'Toyota Sequoia SUV 2012',\n",
              " '189': 'Volkswagen Beetle Hatchback 2012',\n",
              " '19': 'Audi S5 Coupe 2012',\n",
              " '190': 'Volkswagen Golf Hatchback 1991',\n",
              " '191': 'Volkswagen Golf Hatchback 2012',\n",
              " '192': 'Volvo 240 Sedan 1993',\n",
              " '193': 'Volvo C30 Hatchback 2012',\n",
              " '194': 'Volvo XC90 SUV 2007',\n",
              " '195': 'smart fortwo Convertible 2012',\n",
              " '2': 'Acura RL Sedan 2012',\n",
              " '20': 'Audi S6 Sedan 2011',\n",
              " '21': 'Audi TT Hatchback 2011',\n",
              " '22': 'Audi TT RS Coupe 2012',\n",
              " '23': 'Audi TTS Coupe 2012',\n",
              " '24': 'Audi V8 Sedan 1994',\n",
              " '25': 'BMW 1 Series Convertible 2012',\n",
              " '26': 'BMW 1 Series Coupe 2012',\n",
              " '27': 'BMW 3 Series Sedan 2012',\n",
              " '28': 'BMW 3 Series Wagon 2012',\n",
              " '29': 'BMW 6 Series Convertible 2007',\n",
              " '3': 'Acura TL Sedan 2012',\n",
              " '30': 'BMW ActiveHybrid 5 Sedan 2012',\n",
              " '31': 'BMW M3 Coupe 2012',\n",
              " '32': 'BMW M5 Sedan 2010',\n",
              " '33': 'BMW M6 Convertible 2010',\n",
              " '34': 'BMW X3 SUV 2012',\n",
              " '35': 'BMW X5 SUV 2007',\n",
              " '36': 'BMW X6 SUV 2012',\n",
              " '37': 'BMW Z4 Convertible 2012',\n",
              " '38': 'Bentley Arnage Sedan 2009',\n",
              " '39': 'Bentley Continental Flying Spur Sedan 2007',\n",
              " '4': 'Acura TL Type-S 2008',\n",
              " '40': 'Bentley Continental GT Coupe 2007',\n",
              " '41': 'Bentley Continental GT Coupe 2012',\n",
              " '42': 'Bentley Continental Supersports Conv. Convertible 2012',\n",
              " '43': 'Bentley Mulsanne Sedan 2011',\n",
              " '44': 'Bugatti Veyron 16.4 Convertible 2009',\n",
              " '45': 'Bugatti Veyron 16.4 Coupe 2009',\n",
              " '46': 'Buick Enclave SUV 2012',\n",
              " '47': 'Buick Rainier SUV 2007',\n",
              " '48': 'Buick Regal GS 2012',\n",
              " '49': 'Buick Verano Sedan 2012',\n",
              " '5': 'Acura TSX Sedan 2012',\n",
              " '50': 'Cadillac CTS-V Sedan 2012',\n",
              " '51': 'Cadillac Escalade EXT Crew Cab 2007',\n",
              " '52': 'Cadillac SRX SUV 2012',\n",
              " '53': 'Chevrolet Avalanche Crew Cab 2012',\n",
              " '54': 'Chevrolet Camaro Convertible 2012',\n",
              " '55': 'Chevrolet Cobalt SS 2010',\n",
              " '56': 'Chevrolet Corvette Convertible 2012',\n",
              " '57': 'Chevrolet Corvette Ron Fellows Edition Z06 2007',\n",
              " '58': 'Chevrolet Corvette ZR1 2012',\n",
              " '59': 'Chevrolet Express Cargo Van 2007',\n",
              " '6': 'Acura ZDX Hatchback 2012',\n",
              " '60': 'Chevrolet Express Van 2007',\n",
              " '61': 'Chevrolet HHR SS 2010',\n",
              " '62': 'Chevrolet Impala Sedan 2007',\n",
              " '63': 'Chevrolet Malibu Hybrid Sedan 2010',\n",
              " '64': 'Chevrolet Malibu Sedan 2007',\n",
              " '65': 'Chevrolet Monte Carlo Coupe 2007',\n",
              " '66': 'Chevrolet Silverado 1500 Classic Extended Cab 2007',\n",
              " '67': 'Chevrolet Silverado 1500 Extended Cab 2012',\n",
              " '68': 'Chevrolet Silverado 1500 Hybrid Crew Cab 2012',\n",
              " '69': 'Chevrolet Silverado 1500 Regular Cab 2012',\n",
              " '7': 'Aston Martin V8 Vantage Convertible 2012',\n",
              " '70': 'Chevrolet Silverado 2500HD Regular Cab 2012',\n",
              " '71': 'Chevrolet Sonic Sedan 2012',\n",
              " '72': 'Chevrolet Tahoe Hybrid SUV 2012',\n",
              " '73': 'Chevrolet TrailBlazer SS 2009',\n",
              " '74': 'Chevrolet Traverse SUV 2012',\n",
              " '75': 'Chrysler 300 SRT-8 2010',\n",
              " '76': 'Chrysler Aspen SUV 2009',\n",
              " '77': 'Chrysler Crossfire Convertible 2008',\n",
              " '78': 'Chrysler PT Cruiser Convertible 2008',\n",
              " '79': 'Chrysler Sebring Convertible 2010',\n",
              " '8': 'Aston Martin V8 Vantage Coupe 2012',\n",
              " '80': 'Chrysler Town and Country Minivan 2012',\n",
              " '81': 'Daewoo Nubira Wagon 2002',\n",
              " '82': 'Dodge Caliber Wagon 2007',\n",
              " '83': 'Dodge Caliber Wagon 2012',\n",
              " '84': 'Dodge Caravan Minivan 1997',\n",
              " '85': 'Dodge Challenger SRT8 2011',\n",
              " '86': 'Dodge Charger SRT-8 2009',\n",
              " '87': 'Dodge Charger Sedan 2012',\n",
              " '88': 'Dodge Dakota Club Cab 2007',\n",
              " '89': 'Dodge Dakota Crew Cab 2010',\n",
              " '9': 'Aston Martin Virage Convertible 2012',\n",
              " '90': 'Dodge Durango SUV 2007',\n",
              " '91': 'Dodge Durango SUV 2012',\n",
              " '92': 'Dodge Journey SUV 2012',\n",
              " '93': 'Dodge Magnum Wagon 2008',\n",
              " '94': 'Dodge Ram Pickup 3500 Crew Cab 2010',\n",
              " '95': 'Dodge Ram Pickup 3500 Quad Cab 2009',\n",
              " '96': 'Dodge Sprinter Cargo Van 2009',\n",
              " '97': 'Eagle Talon Hatchback 1998',\n",
              " '98': 'FIAT 500 Abarth 2012',\n",
              " '99': 'FIAT 500 Convertible 2012'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKdITaSNtYdh"
      },
      "source": [
        "## Load testing filename list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3b7uaPOy385"
      },
      "source": [
        "import os\n",
        "img_path = '/content/drive/My Drive/NCTU-1091深度學習/HW1/dataset/testing_data'\n",
        "imgidlist = os.listdir(img_path+'/testing_data')\n",
        "for i in range(len(imgidlist)):\n",
        "  imgidlist[i] = int(imgidlist[i].rstrip('.jpg'))\n",
        "imgidlist.sort()\n",
        "for j in range(len(imgidlist)):\n",
        "  imgidlist[j] = str(imgidlist[j]).rjust(6, '0')\n",
        "print(imgidlist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdM7HSgxtiyA"
      },
      "source": [
        "## Load the model and do batch inference and write the corresponding filename and class into csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmjVOodWGtj0",
        "outputId": "a31e3806-5ffb-4c89-afed-0d8b225b63c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, models\n",
        "import torch.nn.functional as F\n",
        "%cd '/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training'\n",
        "model_path = '/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/model.pth'\n",
        "batch_size = 20\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net = torch.load('/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/model.pth').to(device)\n",
        "net.eval()\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((550, 550)),\n",
        "    transforms.CenterCrop(448),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "testset = torchvision.datasets.ImageFolder(root=img_path, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "mark = 0\n",
        "with torch.no_grad():\n",
        "  with open('/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training/bird/predictions2.csv', 'w', newline='') as csf:\n",
        "    writer = csv.writer(csf)\n",
        "    writer.writerow(['id', 'label'])\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "      if use_cuda:\n",
        "          inputs = inputs.to(device)\n",
        "      inputs = Variable(inputs)\n",
        "      output_1, output_2, output_3, output_concat = net(inputs)\n",
        "      outputs_com = output_1 + output_2 + output_3 + output_concat\n",
        "      _, predicted_com = torch.max(outputs_com.data, 1)\n",
        "      for i in range(mark, mark+len(predicted_com)):\n",
        "        writer.writerow([imgidlist[i], clidx_map[str(predicted_com.cpu().numpy()[i % batch_size])]])\n",
        "      print('batch ', batch_idx, ' complete!')\n",
        "      mark += batch_size\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NCTU-1091深度學習/HW1/PMG-Progressive-Multi-Granularity-Training\n",
            "batch  0  complete!\n",
            "batch  1  complete!\n",
            "batch  2  complete!\n",
            "batch  3  complete!\n",
            "batch  4  complete!\n",
            "batch  5  complete!\n",
            "batch  6  complete!\n",
            "batch  7  complete!\n",
            "batch  8  complete!\n",
            "batch  9  complete!\n",
            "batch  10  complete!\n",
            "batch  11  complete!\n",
            "batch  12  complete!\n",
            "batch  13  complete!\n",
            "batch  14  complete!\n",
            "batch  15  complete!\n",
            "batch  16  complete!\n",
            "batch  17  complete!\n",
            "batch  18  complete!\n",
            "batch  19  complete!\n",
            "batch  20  complete!\n",
            "batch  21  complete!\n",
            "batch  22  complete!\n",
            "batch  23  complete!\n",
            "batch  24  complete!\n",
            "batch  25  complete!\n",
            "batch  26  complete!\n",
            "batch  27  complete!\n",
            "batch  28  complete!\n",
            "batch  29  complete!\n",
            "batch  30  complete!\n",
            "batch  31  complete!\n",
            "batch  32  complete!\n",
            "batch  33  complete!\n",
            "batch  34  complete!\n",
            "batch  35  complete!\n",
            "batch  36  complete!\n",
            "batch  37  complete!\n",
            "batch  38  complete!\n",
            "batch  39  complete!\n",
            "batch  40  complete!\n",
            "batch  41  complete!\n",
            "batch  42  complete!\n",
            "batch  43  complete!\n",
            "batch  44  complete!\n",
            "batch  45  complete!\n",
            "batch  46  complete!\n",
            "batch  47  complete!\n",
            "batch  48  complete!\n",
            "batch  49  complete!\n",
            "batch  50  complete!\n",
            "batch  51  complete!\n",
            "batch  52  complete!\n",
            "batch  53  complete!\n",
            "batch  54  complete!\n",
            "batch  55  complete!\n",
            "batch  56  complete!\n",
            "batch  57  complete!\n",
            "batch  58  complete!\n",
            "batch  59  complete!\n",
            "batch  60  complete!\n",
            "batch  61  complete!\n",
            "batch  62  complete!\n",
            "batch  63  complete!\n",
            "batch  64  complete!\n",
            "batch  65  complete!\n",
            "batch  66  complete!\n",
            "batch  67  complete!\n",
            "batch  68  complete!\n",
            "batch  69  complete!\n",
            "batch  70  complete!\n",
            "batch  71  complete!\n",
            "batch  72  complete!\n",
            "batch  73  complete!\n",
            "batch  74  complete!\n",
            "batch  75  complete!\n",
            "batch  76  complete!\n",
            "batch  77  complete!\n",
            "batch  78  complete!\n",
            "batch  79  complete!\n",
            "batch  80  complete!\n",
            "batch  81  complete!\n",
            "batch  82  complete!\n",
            "batch  83  complete!\n",
            "batch  84  complete!\n",
            "batch  85  complete!\n",
            "batch  86  complete!\n",
            "batch  87  complete!\n",
            "batch  88  complete!\n",
            "batch  89  complete!\n",
            "batch  90  complete!\n",
            "batch  91  complete!\n",
            "batch  92  complete!\n",
            "batch  93  complete!\n",
            "batch  94  complete!\n",
            "batch  95  complete!\n",
            "batch  96  complete!\n",
            "batch  97  complete!\n",
            "batch  98  complete!\n",
            "batch  99  complete!\n",
            "batch  100  complete!\n",
            "batch  101  complete!\n",
            "batch  102  complete!\n",
            "batch  103  complete!\n",
            "batch  104  complete!\n",
            "batch  105  complete!\n",
            "batch  106  complete!\n",
            "batch  107  complete!\n",
            "batch  108  complete!\n",
            "batch  109  complete!\n",
            "batch  110  complete!\n",
            "batch  111  complete!\n",
            "batch  112  complete!\n",
            "batch  113  complete!\n",
            "batch  114  complete!\n",
            "batch  115  complete!\n",
            "batch  116  complete!\n",
            "batch  117  complete!\n",
            "batch  118  complete!\n",
            "batch  119  complete!\n",
            "batch  120  complete!\n",
            "batch  121  complete!\n",
            "batch  122  complete!\n",
            "batch  123  complete!\n",
            "batch  124  complete!\n",
            "batch  125  complete!\n",
            "batch  126  complete!\n",
            "batch  127  complete!\n",
            "batch  128  complete!\n",
            "batch  129  complete!\n",
            "batch  130  complete!\n",
            "batch  131  complete!\n",
            "batch  132  complete!\n",
            "batch  133  complete!\n",
            "batch  134  complete!\n",
            "batch  135  complete!\n",
            "batch  136  complete!\n",
            "batch  137  complete!\n",
            "batch  138  complete!\n",
            "batch  139  complete!\n",
            "batch  140  complete!\n",
            "batch  141  complete!\n",
            "batch  142  complete!\n",
            "batch  143  complete!\n",
            "batch  144  complete!\n",
            "batch  145  complete!\n",
            "batch  146  complete!\n",
            "batch  147  complete!\n",
            "batch  148  complete!\n",
            "batch  149  complete!\n",
            "batch  150  complete!\n",
            "batch  151  complete!\n",
            "batch  152  complete!\n",
            "batch  153  complete!\n",
            "batch  154  complete!\n",
            "batch  155  complete!\n",
            "batch  156  complete!\n",
            "batch  157  complete!\n",
            "batch  158  complete!\n",
            "batch  159  complete!\n",
            "batch  160  complete!\n",
            "batch  161  complete!\n",
            "batch  162  complete!\n",
            "batch  163  complete!\n",
            "batch  164  complete!\n",
            "batch  165  complete!\n",
            "batch  166  complete!\n",
            "batch  167  complete!\n",
            "batch  168  complete!\n",
            "batch  169  complete!\n",
            "batch  170  complete!\n",
            "batch  171  complete!\n",
            "batch  172  complete!\n",
            "batch  173  complete!\n",
            "batch  174  complete!\n",
            "batch  175  complete!\n",
            "batch  176  complete!\n",
            "batch  177  complete!\n",
            "batch  178  complete!\n",
            "batch  179  complete!\n",
            "batch  180  complete!\n",
            "batch  181  complete!\n",
            "batch  182  complete!\n",
            "batch  183  complete!\n",
            "batch  184  complete!\n",
            "batch  185  complete!\n",
            "batch  186  complete!\n",
            "batch  187  complete!\n",
            "batch  188  complete!\n",
            "batch  189  complete!\n",
            "batch  190  complete!\n",
            "batch  191  complete!\n",
            "batch  192  complete!\n",
            "batch  193  complete!\n",
            "batch  194  complete!\n",
            "batch  195  complete!\n",
            "batch  196  complete!\n",
            "batch  197  complete!\n",
            "batch  198  complete!\n",
            "batch  199  complete!\n",
            "batch  200  complete!\n",
            "batch  201  complete!\n",
            "batch  202  complete!\n",
            "batch  203  complete!\n",
            "batch  204  complete!\n",
            "batch  205  complete!\n",
            "batch  206  complete!\n",
            "batch  207  complete!\n",
            "batch  208  complete!\n",
            "batch  209  complete!\n",
            "batch  210  complete!\n",
            "batch  211  complete!\n",
            "batch  212  complete!\n",
            "batch  213  complete!\n",
            "batch  214  complete!\n",
            "batch  215  complete!\n",
            "batch  216  complete!\n",
            "batch  217  complete!\n",
            "batch  218  complete!\n",
            "batch  219  complete!\n",
            "batch  220  complete!\n",
            "batch  221  complete!\n",
            "batch  222  complete!\n",
            "batch  223  complete!\n",
            "batch  224  complete!\n",
            "batch  225  complete!\n",
            "batch  226  complete!\n",
            "batch  227  complete!\n",
            "batch  228  complete!\n",
            "batch  229  complete!\n",
            "batch  230  complete!\n",
            "batch  231  complete!\n",
            "batch  232  complete!\n",
            "batch  233  complete!\n",
            "batch  234  complete!\n",
            "batch  235  complete!\n",
            "batch  236  complete!\n",
            "batch  237  complete!\n",
            "batch  238  complete!\n",
            "batch  239  complete!\n",
            "batch  240  complete!\n",
            "batch  241  complete!\n",
            "batch  242  complete!\n",
            "batch  243  complete!\n",
            "batch  244  complete!\n",
            "batch  245  complete!\n",
            "batch  246  complete!\n",
            "batch  247  complete!\n",
            "batch  248  complete!\n",
            "batch  249  complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfAmrVBJ_Lo0"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}